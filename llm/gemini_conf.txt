1. 텍스트 입력

모델에 단일 텍스트 입력 -> 출력

from google import genai

client = genai.Client(api_key="GEMINI_API_KEY")

response = client.models.generate_content(
    model="gemini-2.0-flash",
    contents=["How does AI work?"]
)

model : 모델 이름
contents : 입력 프롬프트

즉, client의 models의 generate_content를 호출하여
내부에 각종 패러미터와 프롬프트를 제공하면 답변이 돌아옴


2. 스트리밍 출력

response = client.models.generate_content_stream(
    model="gemini-2.0-flash",
    contents=["How does AI work?"]
)
for chunk in response:
    print(chunk.text, end="")

3. 멀티턴 대화

chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message("I have 2 dogs in my house.")
print(response.text)

response = chat.send_message("How many paws are in my house?")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)

물론, 여기서도 send_message_stream을 이용하면 스트리밍이 가능하다.

4. 매개변수

from google.genai import types

로 타입을 임포트 한 뒤, generate_content내부에 다음과 같은 코드를
추가함으로서 매개변수 설정이 가능하다.

config=types.GenerateContentConfig(
        max_output_tokens=500,
        temperature=0.1
    )

stopSequences: 출력 생성을 중지하는 문자 시퀀스 집합 (최대 5개)을 지정합니다. 지정하면 stop_sequence가 처음 표시될 때 API가 중지됩니다. 중지 시퀀스는 응답에 포함되지 않습니다.
temperature: 출력의 무작위성을 제어합니다. 창의적인 대답을 위해서는 더 높은 값을 사용하고 확정적인 대답을 위해서는 더 낮은 값을 사용합니다. 값 범위는 [0.0, 2.0]입니다.
maxOutputTokens: 후보에 포함할 최대 토큰 수를 설정합니다.
topP: 모델이 출력용 토큰을 선택하는 방식을 변경합니다. 토큰은 확률의 합계가 topP 값과 같아질 때까지 확률이 가장 높은 순에서 낮은 순으로 선택됩니다. 기본 topP 값은 0.95입니다.
topK: 모델이 출력용 토큰을 선택하는 방식을 변경합니다. topK가 1이면 선택된 토큰이 모델의 어휘에 포함된 모든 토큰 중에서 가장 확률이 높다는 의미이고, topK가 3이면 강도를 사용하여 가장 확률이 높은 3개 토큰 중에서 다음 토큰이 선택된다는 의미입니다. 그런 다음 강도 샘플링을 사용하여 선택된 최종 토큰으로 topP을 기준으로 토큰을 추가로 필터링합니다.

라는 공식 설명이 존재하며,
https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#generationconfig
에서 더 찾아볼 수 있다.

